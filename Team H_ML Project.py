# -*- coding: utf-8 -*-
"""GroupHMachineLearningProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GdiLAWbBbgyQK7cEau0gURelncElizQK
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz 
!tar xf spark-3.1.2-bin-hadoop2.7.tgz

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

!pip install -q findspark
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/My Drive/Colab Notebooks/

df = spark.read.csv('/content/drive/MyDrive/data/yelp_data_unpacked.csv', sep=',',header=True)
df.show()
df.describe()

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/data/yelp_data_unpacked.csv')
df.head()

df['Review_Word_Count'] = df['Review'].apply(lambda x: len(str(x).split()))

df['Review_Word_Count'].plot()
plt.title('Word Counts for Each Yelp Review')
plt.xlabel('Review Number')
plt.ylabel('Number of Words')

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

df['Lowercase'] = df['Review'].apply(lambda x: ' '.join(word.lower() for word in str(x).split()))
df['Cleaned'] = df['Lowercase'].str.replace('[^\w\s]', '')



Reviews_noStopWords = []

for i in range(len(df)):
    
    review = str(df['Cleaned'][i])
    review = review
    review = review.lower()
    
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(review)
    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
    filtered_sentence = []
 
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    
    Reviews_noStopWords.append(filtered_sentence)

df['No_Stop_Words:Tokens'] = Reviews_noStopWords

df['No_Stop_Words:Sentence'] = Reviews_noStopWords
for i in range(len(df)): 
    text_list = df['No_Stop_Words:Tokens'][i]
    blank = ''
    for word in text_list:
        blank = blank+' '+word
    df['No_Stop_Words:Sentence'][i] = blank

Stop_Word_Count = []
for i in range(len(df)):
    review = str(df['Review'][i]).lower()
    count = 0
    for word in review.split():
        if word in stop_words:
            count += 1
    Stop_Word_Count.append(count)

df['Stop_Word_Count'] = Stop_Word_Count
full = df['Review_Word_Count']
partial = df['Stop_Word_Count']

df['Stop_Words_%'] = partial/full
df['Stop_Words_%'].plot()
plt.title('Percentage of Stop Words in Yelp Reviews')
plt.xlabel('Review #')
plt.ylabel('Stop Word Percentage');

pd.Series(''.join(df['No_Stop_Words:Sentence']).split()).value_counts()[:20]

def get_all_text(location):
    all_words = ''
    for i in range(len(df['Name'])):
        if df['Name'][i] == location:
            words = df['No_Stop_Words:Tokens'][i]
            blank = ''
            for word in words:
                blank = blank+' '+word
            all_words = all_words + ' '+blank
            
    return all_words


text = get_all_text("Morris Park Bake Shop")

def plot_cloud(wordcloud):
    plt.figure(figsize=(40, 30))
    plt.imshow(wordcloud) 
    plt.axis("off");

from wordcloud import WordCloud
wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', 
                      colormap='viridis', collocations=False).generate(text)
plot_cloud(wordcloud)

import textblob
from textblob import Word
import nltk
nltk.download('wordnet')

df['Lemmatized'] = df['No_Stop_Words:Sentence'].apply(lambda x: ' '.join(Word(word).lemmatize() for word in x.split()))

import nltk
from nltk.stem.snowball import SnowballStemmer

snowBallStemmer = SnowballStemmer("english")

sentence = df['Lemmatized'][40]
wordList = nltk.word_tokenize(sentence)

stemWords = [snowBallStemmer.stem(word) for word in wordList]

stemmed = ' '.join(stemWords)
print(sentence)
print('')
print(stemmed)

import re
def word_extraction(sentence):   
    words = re.sub("[^\w]", " ",  sentence).split()    
    cleaned_text = [w.lower() for w in words]    
    return cleaned_text

stemmed_list = []
stemmed_tokens_list = []
for i in range(len(df)):
    sentence = df['Lemmatized'][i]
    wordList = nltk.word_tokenize(sentence)
    stemWords = [snowBallStemmer.stem(word) for word in wordList]
    
    stemmed = ' '.join(stemWords)
    stemmed_list.append(stemmed)

df['Stemmed:Sentence'] = stemmed_list

for i in range(len(df)):
    stemmed = df['Stemmed:Sentence'][i]
    tokens = word_extraction(stemmed)
    stemmed_tokens_list.append(tokens)

df['Stemmed:Tokens'] = stemmed_tokens_list

df['Stemmed:Tokens'][1]

df['Stemmed_Word_Count'] = df['Stemmed:Sentence'].apply(lambda x: len(str(x).split()))

plt.figure(figsize=(10,5))
df[df['Review_Word_Count']< 2000]['Review_Word_Count'].plot()
df[df['Stemmed_Word_Count']< 1000]['Stemmed_Word_Count'].plot()
plt.title('Review Word Count, Stemmed with No Stop Words')
plt.xlabel('Review #')
plt.ylabel('Number of Words')
plt.legend()

file = open('/content/drive/MyDrive/data/yelp_data_unpacked.csv', "r")
content = file.read()
pos_lex = content.splitlines()
pos_lex_stem = [snowBallStemmer.stem(word) for word in pos_lex]

good_count = []
for i in range(len(df)):
    count = 0
    tokens = df['Stemmed:Tokens'][i]
    for word in tokens:
        if word in pos_lex_stem:
            count += 1
    good_count.append(count)

df['Positive_Word_Count'] = good_count
df['Positive_Words_%'] = df['Positive_Word_Count']/(df['Review_Word_Count']-df['Stop_Word_Count'])

bad_review = list(df[df['Rating'] < 2.5]['Name'])[0]
bad_index = list(df['Name'].unique()).index(bad_review)

plt.figure(figsize=(10,5))
df[df['Review_Word_Count']< 2000]['Review_Word_Count'].plot()
df[df['Positive_Word_Count']< 600]['Positive_Word_Count'].plot()
plt.title('Positive Words in Yelp Reviews')
plt.xlabel('Review #')
plt.ylabel('Number of Words')
plt.legend()

from textblob import TextBlob
df['Polarity'] = df['Lemmatized'].apply(lambda x: TextBlob(x).sentiment[0])
df['Subjectivity'] = df['Lemmatized'].apply(lambda x: TextBlob(x).sentiment[1])

pol = df[df['Name'] == bad_review]['Polarity'].mean()
pol

df1 = df[df['Polarity'] < 0]
df2 = df[df['Polarity'] > 0]
df3 = df[df['Polarity'] == 0]

df1 = df1.sort_values(by='Polarity')
df2 = df2.sort_values(by='Polarity')
df3 = df3.sort_values(by='Polarity')

df1 = df1.reset_index()
df2 = df2.reset_index()
df3 = df3.reset_index()

df3.index=np.arange(len(df1),(len(df1)+len(df3)))
df2.index=np.arange((len(df3)+len(df1)),(len(df1)+len(df3)+len(df2)))

df4 = df
df4 = df4.sort_values(by='Polarity')

import matplotlib.transforms as transforms


# set up figure
fig, ax = plt.subplots(figsize=(16,8))

x = df.index

# set x axis for partial plots
x1 = df1.index
y1 = df1['Positive_Words_%']
x2 = df2.index
y2 = df2['Positive_Words_%']
x3 = df3.index
y3 = df3['Positive_Words_%']

# calculate average for each partial
av1 = df1['Positive_Words_%'].mean()
av2 = df2['Positive_Words_%'].mean()
av3 = df3['Positive_Words_%'].mean()

# plot word percentage
plt.plot(x1,y1,label='Positive Word Percentage (Polarity < 0)')
plt.plot(x2,y2,label='Positive Word Percentage (Polarity > 0)')
plt.plot(x3,y3,label='Positive Word Percentage (Polarity = 0)')

# plot area to define polarity zones
plt.axvspan(0,len(df1), facecolor='blue', alpha=0.2,label='Polarity < 0')
plt.axvspan(len(df1),(len(df1)+len(df3)), facecolor='green',alpha=0.2,label='Polarity > 0')
plt.axvspan((len(df1)+len(df3)),(len(df1)+len(df3)+len(df2)), facecolor='orange',alpha=0.2, label='Polarity = 0')

# plot average lines
plt.axhline(y=av1,linewidth=3, color='b',label='Average (Polarity < 0): '+str(av1)[2:4]+'%')
plt.axhline(y=av2,linewidth=3, color='r',label='Average (Polarity > 0): '+str(av2)[2:4]+'%')
plt.axhline(y=av3,linewidth=3, color='g',label='Average (Polarity = 0): '+str(av3)[2:4]+'%')

# label x axis as polarity
labels = ['-1','0','0.1','0.2','0.3','0.4','0.5','1']
ticks = [0,len(df1),2655,4539,6869,8704,9924,11065]

ax.set_xticks(ticks)
ax.set_xticklabels(labels)

ax.legend(bbox_to_anchor=(1, 1))
plt.title('Percentage of Positive Words Sorted by Polarity')
plt.xlabel('Polarity')
plt.ylabel('Positive Word Percentage')

df.columns

yelp_data_analyzed = df[['Alias', 'Name', 'Rating', 'Price', 'City', 'Zip_code', 'Address',
       'Review', 'Review_Word_Count', 'No_Stop_Words:Tokens', 'No_Stop_Words:Sentence',
       'Stop_Word_Count', 'Stop_Words_%', 'Lemmatized', 'Stemmed:Sentence',
       'Stemmed:Tokens', 'Polarity','Positive_Word_Count', 'Positive_Words_%','Stemmed_Word_Count']]

yelp_data_analyzed.to_csv(r'yelp_data_analyzed.csv',index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

yelp_data = pd.read_csv('yelp_data_analyzed.csv')

yelp_data_s = yelp_data[yelp_data['Polarity'] != 0]
yelp_data_s = yelp_data_s.reset_index()

sentiment = []
for i in range(len(yelp_data_s)):
    if yelp_data_s['Polarity'][i] > 0:
        sentiment.append('Positive')
    if yelp_data_s['Polarity'][i] < 0:
        sentiment.append('Negative')
yelp_data_s['sentiment'] = sentiment

yelp_data_s['sentiment'].value_counts().plot(kind='bar')
plt.title('Assigned Sentiment of Training Data');

sentiment = []
for i in range(len(yelp_data_s)):
    if yelp_data_s['Polarity'][i] >= 0.4:
        sentiment.append('Positive')
    if yelp_data_s['Polarity'][i] > 0.2 and yelp_data_s['Polarity'][i] < 0.4:
        sentiment.append('Slightly Positive')
    if yelp_data_s['Polarity'][i] <= 0.2 and yelp_data_s['Polarity'][i] > 0:
        sentiment.append('Slightly Negative')
    if yelp_data_s['Polarity'][i] < 0:
        sentiment.append('Negative')
yelp_data_s['sentiment'] = sentiment

yelp_data_s['sentiment'].value_counts().plot(kind='bar')
plt.title('Assigned Sentiment of Training Data');

yelp_data_s = yelp_data_s.rename(columns={'Stop_Words_%':'Stop_Words_P'})
yelp_data_s = yelp_data_s.rename(columns={'Positive_Words_%':'Positive_Words_P'})

fig, axs = plt.subplots(1, 4, figsize = (15,5), sharex = True, sharey = True)
axs[0].hist(yelp_data_s[yelp_data_s.sentiment == 'Positive'].Stop_Words_P, bins = 30, density = True, range = (0,0.5), color = 'green', label = "Positive")
axs[0].axvline(np.median(yelp_data_s[yelp_data_s.sentiment == 'Positive'].Stop_Words_P), color = 'b', lw = 2, )
axs[0].legend(loc="upper right")

axs[1].hist(yelp_data_s[yelp_data_s.sentiment == 'Slightly Positive'].Stop_Words_P, bins = 30, density = True, range = (0,0.5), color = 'purple', label = "Slightly Positive")
axs[1].axvline(np.median(yelp_data_s[yelp_data_s.sentiment == 'Slightly Positive'].Stop_Words_P), color = 'b', lw = 2, )
axs[1].legend(loc="upper right")

axs[2].hist(yelp_data_s[yelp_data_s.sentiment == 'Slightly Negative'].Stop_Words_P, bins = 30, density = True, range = (0,0.5), color = 'orange', label = "Slightly Negative")
axs[2].axvline(np.median(yelp_data_s[yelp_data_s.sentiment == 'Slightly Negative'].Stop_Words_P), color = 'b', lw = 2, )
axs[2].legend(loc="upper right")

axs[3].hist(yelp_data_s[yelp_data_s.sentiment == 'Negative'].Stop_Words_P, bins = 30, density = True, range = (0,0.5), color = 'red', label = "Negative")
axs[3].axvline(np.median(yelp_data_s[yelp_data_s.sentiment == 'Negative'].Stop_Words_P), color = 'b', lw = 2, )
axs[3].legend(loc="upper right")

for ax in axs.flat:
    ax.set(xlabel='Percentage of Stop Words', ylabel='Frequency')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

text = yelp_data_s['No_Stop_Words:Sentence'][0:2]
vectorizer = CountVectorizer()
vectorizer.fit(text)
vector = vectorizer.transform(text)

#summary
print('List of unique Words: ',vectorizer.vocabulary_)
print('')
print('Shape of the Sparse Matrix ',vector.shape)
print('')
print(vector.toarray())



lists = vectorizer.vocabulary_.items() # sorted by key, return a list of tuples

x, y = zip(*lists) # unpack a list of pairs into two tuples

plt.figure(figsize=(12,5))
plt.bar(x, y)
plt.xticks(rotation='vertical')
plt.title('Word Frequency in 2 Yelp Reviews')
plt.ylabel('Word Count')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer

text = yelp_data_s['No_Stop_Words:Sentence'][0:2]
Tvectorizer = TfidfVectorizer()
Tvectorizer.fit(text)
Tvector = Tvectorizer.transform(text)

#summary
print('List of unique Words: ',Tvectorizer.vocabulary_)
print('')
print('Shape of the Sparse Matrix ',Tvector.shape)
print('')
print(Tvector.toarray())

text = yelp_data_s['Stemmed:Sentence']
Tvectorizer = TfidfVectorizer()
Tvectorizer.fit(text)
Tvector = Tvectorizer.transform(text)

voc = Tvectorizer.vocabulary_
df = pd.DataFrame(Tvector.toarray(),columns=voc)

col_name1 ='Name'
col_name2 = 'Review'
col_name3 = 'Polarity'
col_name4 = 'Sentiment'
col_name5 = 'Positive_Words_P'

col1 = yelp_data_s['Name']
col2 = yelp_data_s['Review']
col3 = yelp_data_s['Polarity']
col4 = yelp_data_s['sentiment']
col5 = yelp_data_s['Positive_Words_P']

df.insert(0,col_name1,col1)
df.insert(1,col_name2,col2)
df.insert(2,col_name3,col3)
df.insert(3,col_name4,col4)
df.insert(4,col_name5,col5)



morDf = df[df['Name'] == "Morris Park Bake Shop"]

print(morDf.iloc[15]['Polarity'])
print('Sentiment:', morDf.iloc[15]['Sentiment'])
x = morDf.iloc[15]['chocol':].sort_values(ascending=False)[:10]
x



!pip install pycm

import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from pycm import *

X = df.iloc[0:,4:]
y = df.Sentiment

Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,random_state=0,test_size=0.3)

clf2 = LogisticRegression(solver = 'lbfgs')
model = Pipeline([('classifier',clf2)])
model.fit(Xtrain, ytrain)
predictions = model.predict(Xtest)
mat = confusion_matrix(predictions,ytest)
cm_df =  pd.DataFrame(mat, index= [i for i in ['Negative','Positive',
                                               'Slightly Negative',
                                              'Slightly Positive']],
                     columns= [i for i in ['Negative','Positive',
                                               'Slightly Negative',
                                              'Slightly Positive']])
plt.figure(figsize=(10,10))
sns.heatmap(cm_df, annot=True,cmap='Blues',fmt='g')

cm = ConfusionMatrix(actual_vector=list(ytest), predict_vector=list(predictions))
print('Accuracy Score: ',accuracy_score(predictions,ytest))
print('')
cm.stat(summary=True)